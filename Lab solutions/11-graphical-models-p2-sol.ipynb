{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Statistical Machine Learning - Tutorial\n",
    "\n",
    "### Assumed knowledge\n",
    "- Undirected graphical models (lectures)\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "- Basic operations and definitions of graphical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of the tutorial, we will talk about Markov random fields (MRFs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Random Fields\n",
    "\n",
    "Markov random fields (MRFs) are similar to Bayesian networks (BNs), but with undirected edges. This means that the causal relationships in BNs are no longer defined in MRFs. The relationship between nodes connected by an edge is only \"correlational\" now.\n",
    "\n",
    "Consider an undirected grapical model. Let $C$ denote a *clique* of the graph, corresponding to the set of variables $\\mathsf{x}_C$. We assign a *potential function* $\\psi_C(\\mathsf{x}_C) \\geq 0$ to each set. The joint distribution of all variables in that graph is \n",
    "$$\n",
    "p(\\mathsf{x}) = \\frac{1}{Z} \\prod_{C} \\psi_C(\\mathsf{x}_C),\n",
    "$$\n",
    "where $Z = \\sum_{x}\\prod_{C} \\psi_C(\\mathsf{x}_C)$ is the *normalising constant* to ensure $p(\\mathsf{x})$ sums to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a Bayesian net to an MRF\n",
    "\n",
    "To convert a BN into an MRF, we perform the following steps:\n",
    "1. \"Marry\" the parents: add additional undirected edges between all pairs of parents for each node in the graph. This step is also called \"moralisation\".\n",
    "2. Drop the arrows on all original edges. You should now have an fully undirected graph often called a *moral graph*.\n",
    "3. For each clique in the undirected graph, intialise its potential to 1.\n",
    "4. For each conditional probability in the original graph, multiply it into a corresponding clique potential.\n",
    "5. The normalising constant is simply $Z = 1$.\n",
    "\n",
    "Note that the resulting MRF may represent different conditional independence statements than the original BN.\n",
    "\n",
    "Let's work on one example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** Revisit the Bayesian network we worked on last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://machlearn.gitlab.io/sml2021/tutorials/graphical_model.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://machlearn.gitlab.io/sml2021/tutorials/graphical_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this graph into a moral graph. This corresonds to Steps 1 and 2 above. Draw the resulting network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://machlearn.gitlab.io/sml2021/tutorials/MRF.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution \n",
    "Image(url=\"https://machlearn.gitlab.io/sml2021/tutorials/MRF.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** Answer the following:\n",
    "\n",
    "(a) Identify the maximal cliques in the graph and write down the corresponding clique potentials. This corresponds to Steps 3 and 4 above.\n",
    "\n",
    "(b) Then write out the joint distribution of the undirected graph. \n",
    "\n",
    "(c) Compare the conditional independence statements of the MRF with the BN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "First, the joint distribution of the original directed graph is\n",
    "$$\n",
    "p(A, B, C, D, E) = p(B) p(C) p(A \\mid B) p(D \\mid B, C) p(E \\mid D).\n",
    "$$\n",
    "\n",
    "(a) \n",
    "\n",
    "There are three maximal cliques in this graph: $\\{B, C, D\\}$, $\\{A, B\\}$ and $\\{E, D\\}$. We assign the potentials to these cliques as follows:\n",
    "\n",
    "- $\\psi_1(B,C,D)=p(B)p(C)p(D \\mid B,C)$\n",
    "\n",
    "- $\\psi_2(A,B)=p(A\\mid B)$\n",
    "\n",
    "- $\\psi_3(E,D)=p(E\\mid D)$\n",
    "\n",
    "(Note that other assignments are possible. For example, since both cliques $\\{B, C, D\\}$ and $\\{A, B\\}$ contain node $B$, it is possible to put $p(B)$ within $\\psi_1(B,C,D)$ or $\\psi_2(A,B)$. Similar for node $D$.)\n",
    "\n",
    "(b) \n",
    "\n",
    "Therefore, the joint distribution of the undirected graph is\n",
    "$$\n",
    "p(A, B, C, D, E) = \\psi_1(B,C,D) \\psi_2(A,B) \\psi_3(E,D).\n",
    "$$\n",
    "Note that the normalising constant $Z$ is just 1, because we know that the product of conditional probabilities above is a probability, meaning it sums to 1.\n",
    "\n",
    "(c) \n",
    "\n",
    "In a directed graph, we learned to used D-separation to prove/disprove conditional independence. For example, we have shown that $A \\perp\\!\\!\\!\\perp D \\mid B$, because the only (undirected) path from $A$ to $B$ is $A - D - B$, and $D$ is a TT node that is observed, which makes this path blocked.\n",
    "\n",
    "In an MRF, two nodes are conditionally independent if all paths between them are blocked by observed variables. It is still possible to infer $A \\perp\\!\\!\\!\\perp D \\mid B$, because both paths from $A$ to $B$ contain node $D$, which is observed.\n",
    "\n",
    "However, some conditional independencies cannot be retained. For example, we have shown that in the directed graph, $B \\perp\\!\\!\\!\\perp C \\mid \\emptyset$. This can no longer be inferred from the undirected graph, because $B$ and $C$ are connected by an edge, so they must be in a same clique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum-Product Algorithm\n",
    "\n",
    "The aim of this exercise is to implement the sum product algorithm on a chain graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor graphs\n",
    "\n",
    "Revise the definition of a factor graph in part 3 of the lectures (or Section 8.4.3 of the textbook). A nice property about factor graphs is that the joint distribution can can be expressed as a product of factors. This is important later when we revisit the sum-product algorithm.\n",
    "\n",
    "Here we remind you of how to convert a graph (directed or undirected) into a factor graph.\n",
    "\n",
    "To convert a directed graph into a factor graph:\n",
    "1. Add a factor node corresponding to each conditional probability.\n",
    "2. Assign a conditional probability to the value of its corresponding factor.\n",
    "3. Connect a factor to its corresponding nodes in the conditional probability. \n",
    "\n",
    "To convert an undirected graph into a factor graph:\n",
    "1. Add a factor node corresponding to each maximal clique.\n",
    "2. Create a factor for $Z$, which is over an empty set of variables.\n",
    "3. Assign a clique potential to the value of its corresponding factor.\n",
    "4. Connect a factor to its corersponding nodes in the original clique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributive law\n",
    "\n",
    "The [distributive property](http://en.wikipedia.org/wiki/Distributive_property) can be used to save computation, and is the basis of message passing and dynamic programming. See an [anecdote](http://bibiserv.techfak.uni-bielefeld.de/dynprog/node3_mn.html) about camels.\n",
    "\n",
    "**Exercise 3.** Consider the following equation:\n",
    "$$\n",
    "2*3 + 2*5 = 2 * (3+5).\n",
    "$$\n",
    "\n",
    "* How many mathematical operations (multiplications and additions) are on the left hand side?\n",
    "* How many mathematical operations are on the right hand side?\n",
    "\n",
    "Construct a larger example where there is even more computational savings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "* LHS has 2 multiplies and 1 addition\n",
    "* RHS has 1 multiply and 1 addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message passing\n",
    "\n",
    "Consider the following factor graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://machlearn.gitlab.io/sml2021/tutorials/message_passing.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://machlearn.gitlab.io/sml2021/tutorials/message_passing.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factors are given by the following tables:\n",
    "\n",
    "|f(A,B)  | A=$\\square$ | A=$\\bigcirc$ | A = $\\clubsuit$ | A = $\\heartsuit$ | A = $\\triangle$ |\n",
    "|--|:--:|:--:|:--:|:--:|:--:|\n",
    "|**B**=$p$|0.01|0.01|0.12|0.01|0.14|\n",
    "|**B**=$q$|0.03|0.15|0.01|0.01|0.01|\n",
    "|**B**=$r$|0.13|0.11|0.07|0.18|0.01|\n",
    "\n",
    "|g(B,C) | B=$p$ | B=$q$ | B=$r$ |\n",
    "|--|:--:|:--:|:--:|\n",
    "|**C**=$w$|0.05|0.06|0.07|\n",
    "|**C**=$x$|0.1|0.3|0.2|\n",
    "|**C**=$y$|0.03|0.02|0.1|\n",
    "|**C**=$z$|0.11|0.15|0.08|\n",
    "\n",
    "|  | h(C) |\n",
    "|--|:--:|\n",
    "|**C**=$w$|1.2|\n",
    "|**C**=$x$|3.2|\n",
    "|**C**=$y$|1.8|\n",
    "|**C**=$z$|2.3|\n",
    "\n",
    "Using the sum product algorithm, compute the marginal distribution of the random variable $B$.\n",
    "\n",
    "*Hint: Note that the factors are not normalised.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The joint distribution according to this factor graph is\n",
    "$$\n",
    "p(A, B, C) = f(A, B) \\cdot g(B, C) \\cdot h(C).\n",
    "$$\n",
    "*(Actually, we must use $\\propto$ instead of $=$ because the factors aren't normalised to sum to 1. However, we can just normalise the final result to have a probability, so this won't be a problem. It's only a slight abuse of notation.)*\n",
    "\n",
    "**1. Determining which messages to compute**\n",
    "\n",
    "The inference problem requires us to find $p(B)$. First, we identify that there are two neighbouring factor nodes of $B$, which are $f(A, B)$ and $g(B, C)$. Therefore, the marginal probability of $B$ is the product of two messages from $f(A, B)$ and $g(B, C)$, written as\n",
    "$$\n",
    "p(B) = \\mu_{f(A, B) \\rightarrow B} \\cdot \\mu_{g(B, C) \\rightarrow B}.\n",
    "$$\n",
    "*(Again, we abuse notation by replacing $\\propto$ with $=$ because these messages may not be normalised. This can be easily fixed by normalising $p(B)$ at the end.)*\n",
    "\n",
    "Now let's call $\\mu_{f(A, B) \\rightarrow B}$ the *forward direction* message (left to right) and $\\mu_{g(B, C) \\rightarrow B}$ the *backward direction* message (right to left). We will compute them separately and multiply them together to get $p(B)$.\n",
    "\n",
    "The message from variable node A\n",
    "$\\mu_{A\\to f(A,B)} = 1$\n",
    "\n",
    "**2. Forward direction** $\\mu_{f(A, B) \\rightarrow B}$\n",
    "\n",
    "To get to $B$ from the left, we need to do two steps:\n",
    "- Go from $A \\rightarrow f(A, B)$. This is a message from from a variable leaf node to a factor node, so by (8.70), we have $\\mu_{A \\rightarrow f(A, B)} = 1$.\n",
    "- Go from $f(A, B) \\rightarrow B$. This is a message from a factor (non-leaf) node to a variable node, so it is:\n",
    "\\begin{align*}\n",
    "\\mu_{f(A,B) \\to B} &= \\sum_{A} f(A,B) \\cdot \\mu_{A \\rightarrow f(A, B)} = \\sum_{A} f(A,B) =\n",
    "\\begin{bmatrix}\n",
    "0.29\\\\\n",
    "0.21\\\\\n",
    "0.5\\\\\n",
    "\\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "**3. Backward direction** $\\mu_{g(B, C) \\rightarrow B}$\n",
    "\n",
    "To get to $B$ from the right, we need to do three steps:\n",
    "- Go from $h(C) \\rightarrow C$. This is a message from a factor leaf node to a variable node, so by (8.71) we have $\\mu_{h(C) \\rightarrow C} = h(C).$\n",
    "- Go from $C \\rightarrow g(B, C).$ This is a message from variable (non-leaf) nodes to a factor node, so it's simply the product of all messages to those variable nodes. There's only one such message, which is $\\mu_{h(C) \\rightarrow C}$. Therefore, $\\mu_{C \\rightarrow g(B, C)} = \\mu_{h(C) \\rightarrow C} = h(C)$.\n",
    "- Go from $g(B, C) \\rightarrow B$. This is a message from a factor (non-leaf) node to a variable node, so it is:\n",
    "\\begin{align*}\n",
    "\\mu_{g(B,C) \\to B} = \\sum_{C} g(B,C) \\cdot \\mu_{C \\rightarrow g(B, C)} = \\sum_{C} g(B,C) \\cdot h(C) =\n",
    "\\begin{bmatrix}\n",
    "0.687\\\\\n",
    "1.413\\\\\n",
    "1.088\\\\\n",
    "\\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "**4. Combine messages from both direction**\n",
    "\n",
    "Therefore, the (unnormalised) marginal probability $p(B)$ is\n",
    "$\n",
    "p(B) = \\mu_{f(A,B) \\to B} \\cdot \\mu_{g(B,C) \\to B}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.19923\\\\\n",
    "0.29673\\\\\n",
    "0.544\\\\\n",
    "\\end{bmatrix}.\n",
    "$\n",
    "\n",
    "To ensure this is actually a probability, we need to normalise it. The final result is\n",
    "$p(B) = \n",
    "\\begin{bmatrix}\n",
    "0.192\\\\\n",
    "0.285\\\\\n",
    "0.523\\\\\n",
    "\\end{bmatrix}.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textbook Questions\n",
    "\n",
    "- Q8.20: Induction on graph structure (recall from MATH1005/6005) (Difficulty $\\star$)\n",
    "- Q8.21: Note typo: it should be $f_s(x_s)$ (Difficulty $\\star\\star$)\n",
    "- Q8.27: Construct example showing greedy method not working (Difficulty $\\star\\star$)\n",
    "- Q8.29: Induction on tree structure (recall from MATH1005/6005) (Difficulty $\\star\\star$)\n",
    "- Extra: Derive eq 8.74 to 8.85 w.r.t Fig 8.51\n",
    "\n",
    "- Q10.2: Solving simulataneous equations (Difficulty $\\star$)\n",
    "- Q10.3: Use lagrangian to enforce normalisation of q (Difficulty $\\star\\star$)\n",
    "- Q10.6: Hint, how to introduce log term for both p and q? (Difficulty $\\star\\star$)\n",
    "\n",
    "- Q2.44: Manipulation with a more complex conjugate to derive the posterior (Difficulty $\\star\\star$)\n",
    "\n",
    "- Q10.7: Rewritting to the form of the respective distributions. Mostly algebra. (Difficulty $\\star\\star$)\n",
    "- Q10.8: What will $b_n$ be approximated as? (Difficulty $\\star$)\n",
    "- Q10.9: Essentially, deriving 10.31 and 10.32 (Difficulty $\\star\\star$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
