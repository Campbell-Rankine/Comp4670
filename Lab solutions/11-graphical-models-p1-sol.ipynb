{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Models: Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Statistical Machine Learning - Tutorial\n",
    "\n",
    "### Assumed knowledge\n",
    "- Directed graphical models (Bayesian networks)\n",
    "- Conditional independence, joint distribution factorisation\n",
    "- D-separation and proving conditional (in)dependence based on D-separation.\n",
    "\n",
    "### After this lab, you should be comfortable with\n",
    "- Basic operations and definitions of graphical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial mainly includes three parts: Bayesian Networks (BN), Markov Random Field (MRF) and Sum Product Algorithm (Factor Graph). Before diving into the graphical models, we will first review some basic probability concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Revision\n",
    "\n",
    "Ensure that you are familiar with the following terms:\n",
    "- Joint probability distribution\n",
    "- Marginal distribution\n",
    "- Conditional distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following table defining the joint probability distribution of two variables $A \\in \\{1,2,3,4,5\\}$ and $B \\in \\{p, q, r\\}$.\n",
    "\n",
    "|  | A=$1$ | A=$2$ | A = $3$ | A = $4$ | A = $5$ |\n",
    "|--|:--:|:--:|:--:|:--:|:--:|\n",
    "|**B**=$p$|0.01|0.01|0.12|0.01|0.14|\n",
    "|**B**=$q$|0.03|0.15|0.01|0.01|0.01|\n",
    "|**B**=$r$|0.13|0.11|0.07|0.18|0.01|\n",
    "\n",
    "Below is the table in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table for p(A, B)\n",
      "\n",
      "      1     2     3     4     5\n",
      "p  0.01  0.01  0.12  0.01  0.14\n",
      "q  0.03  0.15  0.01  0.01  0.01\n",
      "r  0.13  0.11  0.07  0.18  0.01\n"
     ]
    }
   ],
   "source": [
    "P_AB = pd.DataFrame([[0.01, 0.01, 0.12, 0.01, 0.14],\n",
    "                     [0.03, 0.15, 0.01, 0.01, 0.01],\n",
    "                     [0.13, 0.11, 0.07, 0.18, 0.01]],\n",
    "                    index=[\"p\", \"q\", \"r\"],\n",
    "                    columns=[1,2,3,4,5])\n",
    "\n",
    "print(\"Table for p(A, B)\\n\")\n",
    "print(P_AB)\n",
    "\n",
    "# Check the table is non-negative\n",
    "assert (P_AB >= 0).all().all()\n",
    "# Check if the table sums to 1\n",
    "assert np.allclose(P_AB.sum().sum(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** Compute the following probabilities:\n",
    "- $p(B)$\n",
    "- $p(B \\mid A = 2)$\n",
    "- $p(B = p \\mid A = 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table for p(B):\n",
      "\n",
      "p    0.29\n",
      "q    0.21\n",
      "r    0.50\n",
      "dtype: float64\n",
      "================================================================================\n",
      "Table for p(B | A=2):\n",
      "\n",
      "p    0.037037\n",
      "q    0.555556\n",
      "r    0.407407\n",
      "Name: 2, dtype: float64\n",
      "================================================================================\n",
      "p(B = p | A = 2) = 0.037037037037037035\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "print('Table for p(B):\\n')\n",
    "print(P_AB.sum(1))\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "P_A_2 = P_AB[2].sum()\n",
    "print('Table for p(B | A=2):\\n')\n",
    "P_B_given_A_2 = P_AB.loc[:, 2] / P_A_2\n",
    "print(P_B_given_A_2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"p(B = p | A = 2) =\", P_B_given_A_2[\"p\"])\n",
    "\n",
    "# print('p(B=p|A=o) =' , P_AB[0,1]/np.sum(P_AB[:,1]))\n",
    "# print('p(B|A=o) =', P_AB[:,1]/np.sum(P_AB[:,1]))\n",
    "# print('p(B) =', np.sum(P_AB, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical verification of Bayes rule\n",
    "\n",
    "Given the joint distribution $p(A,B)$ there are two ways to compute the posterior $p(A \\mid B)$.\n",
    "\n",
    "The first way is by using the definition of conditional probability: $p(A \\mid B) = \\frac{p(A, B)}{p(B)}$.\n",
    "\n",
    "The second way is by using the Bayes rule: $p(A \\mid B) = \\frac{p(B \\mid A)p(A)}{\\sum_A p(A,B)}$.\n",
    "\n",
    "**Exercise 2.** Compute the tables for $p(A \\mid B)$ for all values of $A$ and $B$ using both ways. Ensure that the results are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHS: p(A|B)\n",
      "          1         2         3         4         5\n",
      "p  0.034483  0.034483  0.413793  0.034483  0.482759\n",
      "q  0.142857  0.714286  0.047619  0.047619  0.047619\n",
      "r  0.260000  0.220000  0.140000  0.360000  0.020000\n",
      "RHS:\n",
      "          1         2         3         4         5\n",
      "p  0.034483  0.034483  0.413793  0.034483  0.482759\n",
      "q  0.142857  0.714286  0.047619  0.047619  0.047619\n",
      "r  0.260000  0.220000  0.140000  0.360000  0.020000\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "def joint2cond(P, row_cond=True):\n",
    "    if row_cond:\n",
    "        totals = np.sum(P, axis=0)\n",
    "        return P/totals\n",
    "    else:\n",
    "        totals = np.sum(P, axis=1)\n",
    "        return (P.T/totals).T\n",
    "\n",
    "\n",
    "P_B = np.sum(P_AB, axis=1)\n",
    "P_BgA = joint2cond(P_AB, row_cond=True)\n",
    "P_A = np.sum(P_AB, axis=0)\n",
    "\n",
    "print('LHS: p(A|B)')\n",
    "LHS = joint2cond(P_AB, row_cond=False)\n",
    "print(LHS)\n",
    "\n",
    "numerator = P_BgA * P_A\n",
    "RHS = (numerator.T/P_B).T\n",
    "print('RHS:')\n",
    "print(RHS)\n",
    "\n",
    "assert np.allclose(LHS, RHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent random variables\n",
    "\n",
    "Consider the following 5 random variables.\n",
    "- **A**ches with states (False, True)\n",
    "- **B**ronchitis with states (none, mild, severe)\n",
    "- **C**ough with states (False, True)\n",
    "- **D**isease with states (healthy, carrier, sick, recovering)\n",
    "- **E**mergency with states (False, True)\n",
    "\n",
    "**Exercise 3.** How much memory is needed to store the joint probability distribution if:\n",
    "\n",
    "(a) All variables are dependent?\n",
    "\n",
    "(b) All variables are mutually independent?\n",
    "\n",
    "What does this tell us about the benefit of establishing independencies among a set of random variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "(a) If all variables are dependent, we need to store the joint probability distribution in a 5-dimensional table $p(A, B, C, D, E)$. The total number of entries in this table is $2 \\times 3 \\times 2 \\times 4 \\times 2 = 96$. Because the table must sum to $1$, we only need to store $95$ values in total.\n",
    "\n",
    "(b) If all variables are independent, the joint distribution can be factorised as $p(A,B,C,D,E) = p(A)p(B)p(C)p(D)p(E)$. For each marginal probability with $n$ states, we need to store $n-1$ values (the last value is just 1 minus the stored values). Therefore, the total number of values we need to store is $1 + 2 + 1 + 3 + 1 = 8$.\n",
    "\n",
    "This example illustrates the fact that the space to store a joint distribution grows exponentially with the number of variables (assuming, of course, that the number of states for each variable is the same). With independencies, the space requirement reduces significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Network\n",
    "\n",
    "Bayesian Network is directed graphical model expressing causal relationship between variables.\n",
    "\n",
    "Consider the following graphical model with variables described in Exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://machlearn.gitlab.io/sml2021/tutorials/graphical_model.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://machlearn.gitlab.io/sml2021/tutorials/graphical_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.** Answer the following questions:\n",
    "1. Write down the joint factorisation  for the above graph. \n",
    "2. How much memory is needed to store the joint probability distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. $p(A, B, C, D, E) = p(B) p(C) p(A \\mid B) p(D \\mid B, C) p(E \\mid D)$\n",
    "\n",
    "2. To store each probability on the right-hand side:\n",
    "- $p(B)$ needs 3 - 1 = 2 values \n",
    "- $p(C)$ needs 2 - 1 = 1 values\n",
    "- $p(A \\mid B)$ needs $(2 - 1) \\times 3=3$ values\n",
    "- $p(D \\mid B, C)$ needs $(4 - 1) \\times 3 \\times 2 = 18$ values\n",
    "- $p(E \\mid D)$ needs $(2 - 1) \\times 4 = 4$ values \n",
    "\n",
    "Therefore, the total number of values we need to store is $2 + 1 + 3 + 18 + 4 = 28$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Independence  and D-seperation\n",
    "\n",
    "Conditional independence is an important notion in graphical models. Consider three sets of nodes $X$, $Y$ and $Z$. $X$ is said to be conditionally independent of $Y$, given $Z$ (written as $X \\perp\\!\\!\\!\\perp Y | Z$)  if\n",
    "$$\n",
    "p(X, Y \\mid Z) = p(X \\mid Z) p(Y \\mid Z).\n",
    "$$\n",
    "\n",
    "\n",
    "So, given a joint distribution characterised by a directed graphical model, how do we prove/disprove that two (sets of) variables are independent conditioned on another (set of) variables? The lecture introduced us to two ways of doing so:\n",
    "- From the joint distribution, marginalise all irrelevant variables. We should now have $p(X, Y, Z)$. Then show that, based on the structure of the graph, we can factorise $p(X, Y \\mid Z)$ into $p(X \\mid Z) p(Y \\mid Z)$. An equivalent way is to show $p(X \\mid Y, Z) = p(X \\mid Z)$.\n",
    "- (D-separation) Consider all *undirected* paths from any node in $X$ to any node in $Y$. $X$ is conditionally independent of $Y$ given $Z$ if and only if every such path is *blocked*, that is, the path includes a node such that\n",
    " - The node is in set $Z$ and it is HT or TT, or\n",
    " - The node is HH, but neither the node nor any of its descendants is in set $Z$.\n",
    "\n",
    "**Example proof.**\n",
    "Below is an example to proof using D-separation that in the above graph $C \\perp\\!\\!\\!\\perp E | D$.\n",
    "\n",
    "*(Using the first way)*\n",
    "We will show that $p(E \\mid C, D) = p(E \\mid D)$. Exploiting the structure of the graph, we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(E \\mid C, D) = \\frac{p(E, C, D)}{p(C, D)}.\n",
    "\\end{align*}\n",
    "$$\n",
    "The numerator is\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(E, C, D) & = \\sum_{b} p(E \\mid D) p(D \\mid C, b) p(b) p(C) \\\\\n",
    "& = p(C) p(E \\mid D) \\sum_{b} p(D \\mid C, b) p(b) \\\\\n",
    "& = p(C) p(E \\mid D) \\sum_{b} p(D, b \\mid C) \\\\\n",
    "& = p(C) p(E \\mid D) p(D \\mid C).\n",
    "\\end{align*}\n",
    "$$\n",
    "The denominator is\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(C, D) & = \\sum_{b} p(D \\mid C, b) p(b) p(C) \\\\\n",
    "& = p(C) \\sum_{b} p(D \\mid C, b) p(b) \\\\\n",
    "& = p(C) \\sum_{b} p(D, b \\mid C) \\\\\n",
    "& = p(C) p(D \\mid C).\n",
    "\\end{align*}\n",
    "$$\n",
    "Therefore, $p(E \\mid C, D) = \\frac{p(C) p(E \\mid D) p(D \\mid C)}{p(C) p(D \\mid C)} = p(E \\mid D)$, which by definition implies that $E$ is independent of $C$ conditioned on $D$.\n",
    "\n",
    "*(Using D-separation)*\n",
    "There is only one (undirected) path from $C$ to $E$, which is $C \\rightarrow D \\rightarrow E$. This path contains node $D$, which is observed and is a HT node. Therefore, this path must be blocked. Since all paths between $C$ and $D$ are blocked, $C \\perp\\!\\!\\!\\perp E | D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.** Identify and prove whether the conditional independences holds for the following cases: \n",
    "\n",
    "(a) A and D, when B is observed.\n",
    "\n",
    "(b) B and C, when none of the variables are observed.\n",
    "\n",
    "(c) B and C, when E is observed.\n",
    "\n",
    "(d) A and C, when none of the variables are observed.\n",
    "\n",
    "(e) A and C, when B is observed.\n",
    "\n",
    "(f) A and E, when D is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating distributions for BN\n",
    "\n",
    "**Exercise 6.** Consider the following tables.\n",
    "\n",
    "|p(B) | B=n | B=m | B=s |\n",
    "|:-----:|:--:|:--:|:--:|\n",
    "|marginal| 0.97 | 0.01 | 0.02 |\n",
    "\n",
    "|p(C) | C=False | C=True |\n",
    "|:-----:|:--:|:--:|\n",
    "|marginal| 0.7 | 0.3 |\n",
    "\n",
    "| p(A\\|B) | B=n | B=m | B=s |\n",
    "|:-----:|:--:|:--:|:--:|\n",
    "|**A**=False |0.9|0.8|0.3|\n",
    "|**A**=True |0.1|0.2|0.7|\n",
    "\n",
    "| p(D\\|B,C) | B=n, C=F | B=m, C=F | B=s, C=F | B=n, C=T | B=m, C=T | B=s, C=T |\n",
    "|:-----:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|**D**=healthy   |0.9 |0.8 |0.1 |  0.3 |0.4 |0.01|\n",
    "|**D**=carrier   |0.08|0.17|0.01|  0.05|0.05|0.01|\n",
    "|**D**=sick      |0.01|0.01|0.87|  0.05|0.15|0.97|\n",
    "|**D**=recovering|0.01|0.02|0.02|  0.6 |0.4 |0.01|\n",
    "\n",
    "\n",
    "| p(E\\|D) | D=h | D=c | D=s | D=r |\n",
    "|:-----:|:--:|:--:|:--:|:--:|\n",
    "|**E**=False | 0.99 | 0.99| 0.4| 0.9|\n",
    "|**E**=True | 0.01| 0.01| 0.6| 0.1|\n",
    "\n",
    "Compute the following:\n",
    "- $p(A,B,C,D,E)$\n",
    "- $p(E)$\n",
    "- $p(E \\mid B=s)$\n",
    "- $p(E \\mid B=s, C=T)$\n",
    "\n",
    "Note that there are two ways of arriving at the distributions:\n",
    "1. By computing p(A,B,C,D,E) and marginalising and conditioning appropriately\n",
    "2. By only computing the required distributions directly using the graphical model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The sample solution illustrates the key points of the two ways of computing the distributions.\n",
    "\n",
    "No1. By computing p(A,B,C,D,E) and marginalising and conditioning appropriately\n",
    "\n",
    "* $p(A,B,C,D,E) = p(B)p(C)p(A|B)p(D|B,C)p(E|D)$\n",
    "* $p(E) = \\sum_{a,b,c,d} p(a,b,c,d,E)$\n",
    "* $p(E|B=s) = \\frac{p(E, B =s)}{p(B = s)} = \\frac{\\sum_{a, c, d}p(a, B=s, c, d, E)}{p(B = s)}$\n",
    "* $p(E|B=s, C=T) = \\frac{p(E, B =s, C = T)}{p(B = s, C = T)} = \\frac{\\sum_{a, d}p(a, B =s, C = T, d, E)}{p(B = s, C = T)} = \\frac{\\sum_{a, d}p(a, B =s, C = T, d, E)}{p(B = s) P(C = T)}$\n",
    "\n",
    "No2. By only computing the required distributions directly using the graphical model structure.\n",
    "\n",
    "* $p(A,B,C,D,E) = p(B)p(C)p(A|B)p(D|B,C)p(E|D)$\n",
    "* $p(E) = \\sum_{b,c,d} p(b)p(c)p(d|b,c)p(E|d)$\n",
    "* $p(E|B=s) = \\frac{p(E, B =s)}{ p(B =s)}  =  \\frac{\\sum_{c,d} p(B = s)p(c)p(d|B = s,c)p(E|d)}{p(B =s)} = \\sum_{c,d} p(c)p(d|B = s,c)p(E|d)$\n",
    "* $p(E|B=s, C=T) = \\frac{p(E, B =s, C = T)}{p(B = s, C = T)} = \\frac{\\sum_{d}p(B =s)p(C = T)p(d|B=s, C=T)p(E|d)}{p(B = s) p(C = T)} = \\sum_{d} p(d|B=s, C=T)p(E|d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textbook Questions\n",
    "\n",
    "These exercises also include ones about Markov random fields, which we will cover next week.\n",
    "\n",
    "- Q8.20: Induction on graph structure (recall from MATH1005/6005) (Difficulty $\\star$)\n",
    "- Q8.21: Note typo: it should be $f_s(x_s)$ (Difficulty $\\star\\star$)\n",
    "- Q8.27: Construct example showing greedy method not working (Difficulty $\\star\\star$)\n",
    "- Q8.29: Induction on tree structure (recall from MATH1005/6005) (Difficulty $\\star\\star$)\n",
    "- Extra: Derive eq 8.74 to 8.85 w.r.t Fig 8.51\n",
    "\n",
    "- Q10.2: Solving simulataneous equations (Difficulty $\\star$)\n",
    "- Q10.3: Use lagrangian to enforce normalisation of q (Difficulty $\\star\\star$)\n",
    "- Q10.6: Hint, how to introduce log term for both p and q? (Difficulty $\\star\\star$)\n",
    "\n",
    "- Q2.44: Manipulation with a more complex conjugate to derive the posterior (Difficulty $\\star\\star$)\n",
    "\n",
    "- Q10.7: Rewritting to the form of the respective distributions. Mostly algebra. (Difficulty $\\star\\star$)\n",
    "- Q10.8: What will $b_n$ be approximated as? (Difficulty $\\star$)\n",
    "- Q10.9: Essentially, deriving 10.31 and 10.32 (Difficulty $\\star\\star$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
