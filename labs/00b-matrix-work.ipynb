{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Statistical Machine Learning - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as optz\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors, matrices and operations in Python\n",
    "\n",
    "The `numpy` library allows us to conveniently represent and work with vectors, matrices and tensors in higher dimensions. It supports many operations in linear algebra and will be used in the assignments this semester.\n",
    "\n",
    "If you are unfamiliar with `numpy`, we highly recommend you spend sometime familiarising yourself with it, especially by consulting [its documentation](https://numpy.org/doc/stable/reference/index.html). If you are confident with it, you can skip this section.\n",
    "\n",
    "Vectors and matrices are both represented using an `np.array` object. Typically:\n",
    "\n",
    "- Vectors are one-dimensional arrays: for example, `x = np.array([1,2,3])`\n",
    "- Matrices are two-dimensional arrays: for example, `A = np.array([[1,2,3], [4,5,6], [7,8,9]]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful attributes of `np.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define an array\n",
    "X = np.array([[1,  2,  3,  4],\n",
    "              [5,  6,  7,  8],\n",
    "              [9, 10, 11, 12]],\n",
    "             dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape: this tells you the dimensions of the array\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size: this tells you how many items the array has \n",
    "#       in total\n",
    "X.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dtype: this tells you what format each element of\n",
    "#        the array is stored as\n",
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  9],\n",
       "       [ 2,  6, 10],\n",
       "       [ 3,  7, 11],\n",
       "       [ 4,  8, 12]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T: for 2D matrices, you can easily transpose them\n",
    "X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful methods and operations on `np.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define some arrays\n",
    "A = np.array([[1,  2,  3,  4],\n",
    "              [5,  6,  7,  8],\n",
    "              [9, 10, 11, 12]],\n",
    "             dtype=np.int32)\n",
    "\n",
    "B = np.array([[13,  14,  15,  16],\n",
    "              [17,  18,  19,  20],\n",
    "              [21,  22,  23,  24]],\n",
    "             dtype=np.int32)\n",
    "\n",
    "x = np.array([4, 3, 2, 1])\n",
    "\n",
    "y = np.array([9, 8, 7, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  5],\n",
       "       [ 6,  7,  8,  9],\n",
       "       [10, 11, 12, 13]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Element-wise operation: add 1 to all elements of an array\n",
    "A + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07692308, 0.14285714, 0.2       , 0.25      ],\n",
       "       [0.29411765, 0.33333333, 0.36842105, 0.4       ],\n",
       "       [0.42857143, 0.45454545, 0.47826087, 0.5       ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For two arrays of the same shape, you can add/subtract/... them\n",
    "assert A.shape == B.shape\n",
    "A / B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dot product between two vectors of the same size\n",
    "x.dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 20,  60, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix-vector product\n",
    "assert A.shape[1] == x.shape[0]\n",
    "A @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[150, 190, 230],\n",
       "       [382, 486, 590],\n",
       "       [614, 782, 950]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix-matrix product\n",
    "assert A.shape[1] == B.T.shape[0]\n",
    "A @ B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Be careful**: When working with `np.array`\n",
    "\n",
    "- The operator `*` represents the element-wise product, whereas\n",
    "- The operator `@` represents the matrix product defined in linear algebra.\n",
    "\n",
    "You should choose the right operator for your purpose!\n",
    "\n",
    "**Side notes**: \n",
    "\n",
    "- The `np.dot` function could also be used for dot products, matrix-vector products and matrix-matrix products. So, `A @ x` is equivalent to `np.dot(A, x)`. In fact, the `@` operator is basiscally the `np.matmul` function.\n",
    "- You might have seen another (soon-to-be-deprecated) object in `numpy` called `np.matrix`. This is similar to `np.array`, but in this case both the `*` and `@` operators represent the matrix product. Therefore, the safest choice to make when you want to use the matrix product is `@`, regardless of whether you use `np.array` or `np.matrix`.\n",
    "\n",
    "Now that you are comfortable with `numpy`, let's move on to main dish of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance matrix and positive semidefinite matrix\n",
    "\n",
    "For a dataset $X$ with $N$ examples and $D$ features, we can represent it as a matrix where every column represents a data example.\n",
    "1. What are the dimensions of this matrix $X$? In `numpy`, what is its shape?\n",
    "2. The covariance matrix $C$ is the matrix representing the variance and covariance between each pair of features. Write the formula for calculating $C$. What is the size of this matrix?\n",
    "3. Take a look at the `gen_data` function below. What does it do? Generate a data matrix $X$ using that function. Compute the covariance matrix $C$ and its [eigenvalue decomposition](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html) using `np.linalg.eigh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gen_data(n_samples=100, n_feat=5):\n",
    "    \"\"\"Generate data from two Gaussians\n",
    "    n_samples = number of samples from each Gaussian\n",
    "    n_feat = dimension of the features\n",
    "    \"\"\"\n",
    "    X1 = np.ones((n_feat, n_samples))   + np.random.randn(n_feat, n_samples)\n",
    "    X2 = - np.ones((n_feat, n_samples)) + np.random.randn(n_feat, n_samples)\n",
    "    X  = np.hstack([X1, X2])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[[2.1393456  1.01596772 1.0430628  0.97328084 0.84342769]\n",
      " [1.01596772 1.79213515 0.95797935 0.96053834 0.89753182]\n",
      " [1.0430628  0.95797935 1.97051462 0.99356055 0.9929234 ]\n",
      " [0.97328084 0.96053834 0.99356055 1.85748238 0.95567294]\n",
      " [0.84342769 0.89753182 0.9929234  0.95567294 1.91972759]]\n",
      "eigenvalue decomposition of C: (array([0.85111967, 0.89731894, 0.93895749, 1.19640884, 5.7954004 ]), array([[ 0.24769866,  0.38748141,  0.07314876, -0.75063393, -0.46872326],\n",
      "       [-0.78507216, -0.08241087, -0.42992129, -0.07376223, -0.43196871],\n",
      "       [-0.12477728, -0.43139486,  0.75816297,  0.09872952, -0.46235321],\n",
      "       [ 0.54626295, -0.4993743 , -0.48356861,  0.1514212 , -0.44210307],\n",
      "       [ 0.09129344,  0.63842807,  0.03416501,  0.63121465, -0.42950661]]))\n"
     ]
    }
   ],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate\n",
    "data = gen_data()\n",
    "n = data.shape[1]\n",
    "print(n)\n",
    "\n",
    "#assert statement for correct\n",
    "check = np.cov(data)\n",
    "#retreive means of all columns in \n",
    "ms = np.mean(data, axis=1)\n",
    "\n",
    "#for i, x in enumerate data.columns:\n",
    "Mcols = np.zeros(shape=(5,200))\n",
    "for i, x in enumerate(data):\n",
    "    Mcols[i] = (x-ms[i])\n",
    "\n",
    "cov = (1/(n-1))*(np.dot(Mcols, Mcols.T))\n",
    "\n",
    "print(cov)\n",
    "\n",
    "print(\"eigenvalue decomposition of C: \" + str(np.linalg.eigh(cov)))\n",
    "\n",
    "assert(check.any() == cov.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance matrices are an example of a class of matrices which are *positive semidefinite*. \n",
    "\n",
    "- A matrix $A$ is called symmetric if $A_{ij}=A_{ji}$. Another way to say this is that $A=A^\\top$.\n",
    "- A matrix $A\\in\\mathbb{R}^{n\\times n}$ is called **positive semidefinite**, if for all vectors $x\\in\\mathbb{R}^n$,\n",
    "$$\n",
    "    x^T A x \\geqslant 0.\n",
    "$$\n",
    "\n",
    "Show that the eigenvalues of a positive semidefinite matrix are non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis (PCA)\n",
    "\n",
    "You will see more about principal component analysis later in the course. For now, we will treat PCA as an exercise in matrix manipulation.\n",
    "\n",
    "Given any matrix $X \\in \\mathbb{R}^{m \\times n}$, the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) of $X$ is\n",
    "$$\n",
    "X = U S V^T\n",
    "$$\n",
    "where \n",
    "\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix. The columns of $U$ are called the *left singular vectors* of $X$.\n",
    "- $S \\in \\mathbb{R}^{m \\times n}$ contains the *singular values* of $X$ along its diagonal. All other entries of $S$ are zero.\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix. The rows of $V^T$ are called the *right singular vectors* of $X$.\n",
    "\n",
    "Your task:\n",
    "\n",
    "1. Substitute the singular value decomposition of $X$ into the covariance matrix $C$. Simplify the resulting expression. You should have an expression of $C$ in terms of $U$ and $S$ only.\n",
    "2. Recall the definition of an eigenvalue decomposition. What do you think the columns of $U$ represent with respect to $C$?\n",
    "3. What is the matrix that contains the eigenvectors corresponding to the $k$ largest eigenvalues of $C$?\n",
    "4. Recall that PCA considers the covariance matrix of a data matrix $X$. Using the definition of SVD above, derive expressions for the projection of $X$ onto the $k$ top principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement PCA\n",
    "\n",
    "Implement the `pca` function below. Your function should take two arguments:\n",
    "\n",
    "1. The data matrix of shape `(n_features, n_samples)`\n",
    "2. The number of components to keep `n_pc`\n",
    "\n",
    "and return two matrices:\n",
    "\n",
    "1. The principal components of shape `(n_features, n_pc)`\n",
    "2. The projected matrix of shape `(n_pc, n_samples)`\n",
    "\n",
    "For SVD, you can use the function ```numpy.linalg.svd```.\n",
    "\n",
    "*Do not forget to center the data by removing the mean first.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate\n",
    "\n",
    "def centre(data):\n",
    "    ms = np.mean(data, axis=1)\n",
    "    Mcols = np.zeros(shape=(5,200))\n",
    "    for i, x in enumerate(data):\n",
    "        Mcols[i] = (x-ms[i])\n",
    "    return Mcols\n",
    "\n",
    "def PCA(data, k): #data is of shape (n_features, n_samples) k = n_pc\n",
    "    Mcols = centre(data)\n",
    "    n=data.shape[1]\n",
    "    \n",
    "    cov = (1/(n-1))*(np.dot(Mcols, Mcols.T))\n",
    "    \n",
    "    Ev, Evec = np.linalg.eigh(cov)\n",
    "    print(Evec.shape)\n",
    "    print(Evec[0:k].shape)\n",
    "    \n",
    "    pc_n = Evec[0:k] @ data\n",
    "    \n",
    "    print(pc_n.shape)\n",
    "    \n",
    "    return(Evec[:, k], pc_n)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the toy data generated from the `gen_data` function above. It has 200 examples:\n",
    "\n",
    "- The first 100 examples follow a Gaussian distribution with unit variance, centered at $\\mathbf{1}$\n",
    "- The last 100 examples follow a Gaussian distribution with unit variance, centered at $-\\mathbf{1}$\n",
    "\n",
    "Obtain the projection of the toy data to its first two principal components. Plot the results. You should be able to see that the first principal component already gives you the axis of discrimination. Revisit the question of the effect of dimension on two Gaussians with unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(2, 5)\n",
      "(2, 200)\n"
     ]
    }
   ],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate\n",
    "\n",
    "data = gen_data()\n",
    "\n",
    "pc, pc_n, = PCA(data, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenfaces\n",
    "\n",
    "The aim of this section of the tutorial is to see that in some cases, the principal components can be human interpretable.\n",
    "\n",
    "The dataset we will be using consists of images of Colin Powell, resized to a smaller image, from [LFW](http://vis-www.cs.umass.edu/lfw/). Download the images from [Piazza resources page](https://piazza.com/anu.edu.au/spring2022/comp4670comp8600/resources) stored in the ```lfw_colin.pkl``` file.\n",
    "\n",
    "The images are stored in a 3D array of shape `(n_samples, height, width)`, where `n_samples = 236`, `height = 50` and `width = 37`.\n",
    "\n",
    "1. Reshape the dataset so that its dimensions are `(1850, 236)`, where $1,850 = 50 \\times 37$. Essentially, we are flattening a 2D image into a long vector.\n",
    "2. Use the `pca` function you wrote above to find the top 12 principal components. You should have a matrix of shape `(1850, 12)`.\n",
    "3. Reshape that matrix to `(12, 50, 37)`, so that we have a collection of 12 images, each with a height of 50 pixels and a width of 37 pixels. These images are called the *eigenfaces* in the dataset. Can you explain why they're called like that?\n",
    "4. Use the function `plot_gallery` to plot the 12 eigenfaces. Discuss what each component (eigenface) potentially captures; for example, lighting, shape, orientation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising images\n",
    "def plot_gallery(images, titles, h, w, n_row=2, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "lfw_colin = pickle.load(open('lfw_colin.pkl', 'rb'))\n",
    "\n",
    "# Introspect the images array to find the shapes (for plotting)\n",
    "n_samples, height, width = lfw_colin['images'].shape\n",
    "\n",
    "# Plot the first few images\n",
    "plot_gallery(lfw_colin['images'], range(n_samples), height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
